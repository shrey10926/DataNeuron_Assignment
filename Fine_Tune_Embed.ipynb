{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6293,
     "status": "ok",
     "timestamp": 1707807482248,
     "user": {
      "displayName": "Dhruv Kothiya",
      "userId": "04135816780688131851"
     },
     "user_tz": -330
    },
    "id": "fdtdxb3XEwaN",
    "outputId": "d226eade-6477-4706-d562-7789bb8784bc"
   },
   "outputs": [],
   "source": [
    "!pip install accelerate -U\n",
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "id": "yl3MX-OVJkaI",
    "outputId": "0738a724-58c0-40c0-f8d0-5ebdf835ff8f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This file runs Masked Language Model. You provide a training file. Each line is interpreted as a sentence / paragraph.\n",
    "Optionally, you can also provide a dev file.\n",
    "\n",
    "The fine-tuned model is stored in the output/model_name folder.\n",
    "\n",
    "Usage:\n",
    "python train_mlm.py model_name data/train_sentences.txt [data/dev_sentences.txt]\n",
    "\"\"\"\n",
    "\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "from transformers import DataCollatorForLanguageModeling, DataCollatorForWholeWordMask\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import sys\n",
    "import gzip\n",
    "from datetime import datetime\n",
    "import torch\n",
    "\n",
    "if len(sys.argv) < 3:\n",
    "    print(\"Usage: python train_mlm.py model_name data/train_sentences.txt [data/dev_sentences.txt]\")\n",
    "    exit()\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\" #sys.argv[1]\n",
    "per_device_train_batch_size = 16\n",
    "\n",
    "save_steps = 1000  # Save model every 1k steps\n",
    "num_train_epochs = 20  # Number of epochs\n",
    "use_fp16 = False  # Set to True, if your GPU supports FP16 operations\n",
    "max_length = 512  # Max length for a text input\n",
    "do_whole_word_mask = True  # If set to true, whole words are masked\n",
    "mlm_prob = 0.15  # Probability that a word is replaced by a [MASK] token\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "output_dir = \"output/{}-{}\".format(model_name.replace(\"/\", \"_\"), datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
    "print(\"Save checkpoints to:\", output_dir)\n",
    "\n",
    "\n",
    "##### Load our training datasets\n",
    "\n",
    "train_sentences = []\n",
    "train_path = \"text.txt\" #sys.argv[2]\n",
    "with gzip.open(train_path, \"rt\", encoding=\"utf8\") if train_path.endswith(\".gz\") else open(\n",
    "    train_path, \"r\", encoding=\"utf8\"\n",
    ") as fIn:\n",
    "    for line in fIn:\n",
    "        line = line.strip()\n",
    "        if len(line) >= 10:\n",
    "            train_sentences.append(line)\n",
    "\n",
    "print(\"Train sentences:\", len(train_sentences))\n",
    "\n",
    "dev_sentences = []\n",
    "# if len(sys.argv) >= 4:\n",
    "dev_path = \"text.txt\" #sys.argv[3]\n",
    "with gzip.open(dev_path, \"rt\", encoding=\"utf8\") if dev_path.endswith(\".gz\") else open(\n",
    "    dev_path, \"r\", encoding=\"utf8\"\n",
    ") as fIn:\n",
    "    for line in fIn:\n",
    "        line = line.strip()\n",
    "        if len(line) >= 10:\n",
    "            dev_sentences.append(line)\n",
    "\n",
    "print(\"Dev sentences:\", len(dev_sentences))\n",
    "\n",
    "\n",
    "# A dataset wrapper, that tokenizes our data on-the-fly\n",
    "class TokenizedSentencesDataset:\n",
    "    def __init__(self, sentences, tokenizer, max_length, cache_tokenization=False):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sentences = sentences\n",
    "        self.max_length = max_length\n",
    "        self.cache_tokenization = cache_tokenization\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if not self.cache_tokenization:\n",
    "            return self.tokenizer(\n",
    "                self.sentences[item],\n",
    "                add_special_tokens=True,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_special_tokens_mask=True,\n",
    "            )\n",
    "\n",
    "        if isinstance(self.sentences[item], str):\n",
    "            self.sentences[item] = self.tokenizer(\n",
    "                self.sentences[item],\n",
    "                add_special_tokens=True,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_special_tokens_mask=True,\n",
    "            )\n",
    "        return self.sentences[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "\n",
    "train_dataset = TokenizedSentencesDataset(train_sentences, tokenizer, max_length)\n",
    "dev_dataset = (\n",
    "    TokenizedSentencesDataset(dev_sentences, tokenizer, max_length, cache_tokenization=True)\n",
    "    if len(dev_sentences) > 0\n",
    "    else None\n",
    ")\n",
    "\n",
    "\n",
    "##### Training arguments\n",
    "\n",
    "if do_whole_word_mask:\n",
    "    data_collator = DataCollatorForWholeWordMask(tokenizer=tokenizer, mlm=True, mlm_probability=mlm_prob)\n",
    "else:\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=mlm_prob)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    evaluation_strategy=\"steps\" if dev_dataset is not None else \"no\",\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    eval_steps=save_steps,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=save_steps,\n",
    "    save_total_limit=3,\n",
    "    prediction_loss_only=True,\n",
    "    fp16=use_fp16,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, args=training_args, data_collator=data_collator, train_dataset=train_dataset, eval_dataset=dev_dataset\n",
    ")\n",
    "\n",
    "print(\"Save tokenizer to:\", output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"Save model to:\", output_dir)\n",
    "model.save_pretrained(output_dir)\n",
    "\n",
    "print(\"Training done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aRePGf7lLrSs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Bp-eSF6LrQJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6524,
     "status": "ok",
     "timestamp": 1707806661691,
     "user": {
      "displayName": "Dhruv Kothiya",
      "userId": "04135816780688131851"
     },
     "user_tz": -330
    },
    "id": "wLE3qfbLLrLX",
    "outputId": "951ddae3-95a8-46b2-89f4-c3958176b667",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shrey_jain/anaconda3/envs/embed/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of MPNetModel were not initialized from the model checkpoint at output/sentence-transformers_all-mpnet-base-v2-2024-02-13_17-18-11 and are newly initialized: ['mpnet.pooler.dense.bias', 'mpnet.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, models, util\n",
    "\n",
    "\n",
    "word_embedding_model = models.Transformer(\"output/sentence-transformers_all-mpnet-base-v2-2024-02-13_17-18-11\", max_seq_length=512)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/Shrey-Jain/embed'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model = SentenceTransformer(\"output/sentence-transformers_all-mpnet-base-v2-2024-02-13_12-54-59\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model max lenght is :512\n",
      "model max lenght is :512\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "\n",
    "print(f'model max lenght is :{model.max_seq_length}')\n",
    "model.max_seq_length = 512\n",
    "print(f'model max lenght is :{model.max_seq_length}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 1399,
     "status": "ok",
     "timestamp": 1707806665003,
     "user": {
      "displayName": "Dhruv Kothiya",
      "userId": "04135816780688131851"
     },
     "user_tz": -330
    },
    "id": "SIZtipHlL0SW"
   },
   "outputs": [],
   "source": [
    "sentences1 = 'davenport hits out at wimbledon world number one lindsay davenport has criticised wimbledon over the issue of equal prize money for women.  reacting to a disputed comment by all england club chairman tim phillips  the american said:  i think it is highly insulting if prize money is taken away.  somebody  i think it was mr phillips  said they won t have money for flowers at wimbledon. that s insulting.  an all england club spokesperson denied phillips made the remark  insisting:  he definitely didn t say it.  the statement added:  it was said by someone else and was a humorous aside at the end of a radio interview when the conversation had moved to talking about the wimbledon grounds.   davenport was speaking following the announcement that this week s dubai duty free event will join the us and australian opens in offering equal prize money for women.  you hear about women playing only three sets while men play five   said daveport.  and the best women are never going to beat the best men.  but it s a different game you go to watch with the women - it doesn t make it better or worse.  hopefully we will be able to change people s minds.   serena williams  who is also in dubai  added:  i m obviously for equal prize money.  women s tennis is exciting. men s tennis is exciting as well  but the women have it right now.  if you are bringing in the spectators you should be able to reap what everyone else is able to reap.'\n",
    "sentences2 = 'safety alert as gm recalls cars the world s biggest carmaker general motors (gm) is recalling nearly 200 000 vehicles in the us on safety grounds  according to federal regulators.  the national highway traffic safety administration (nhtsa) said the largest recall involves 155 465 pickups  vans and sports utility vehicles (suvs). this is because of possible malfunctions with the braking systems. the affected vehicles in the product recall are from the 2004 and 2005 model years  gm said. those vehicles with potential faults are the chevrolet avalanche  express  kodiak  silverade and suburban; the gmc savana  sierra and yukon.  the nhtsa said a pressure accumulator in the braking system could crack during normal driving and fragments could injure people if the hood was open. this could allow hydraulic fluid to leak  which could make it harder to brake or steer and could cause a crash  it warned. gm is also recalling 19 924 cadillac xlr coupes  srx suvs and pontiac grand prix sedans from the 2004 model year. this is because the accelerator pedal may not work properly in extremely cold temperatures  requiring more braking. in addition  the car giant is calling back 17 815 buick raniers  chevrolet trailblazers  gmc envoys and isuzu ascenders from the 2005 model years because the windshield is not properly fitted and could fall out in a crash. however  gm stressed that it did not know of any injuries related to the problems. news of the recall follows an announcement last month that gm expects earnings this year be lower than in 2004. the world s biggest car maker is grappling with losses in its european business  weak us sales and now a product recall. in january  gm said higher healthcare costs in north america  and lower profits at its financial services subsidiary would hurt its performance in 2005.'\n",
    "\n",
    "embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
    "embeddings2 = model.encode(sentences2, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 615,
     "status": "ok",
     "timestamp": 1707806665616,
     "user": {
      "displayName": "Dhruv Kothiya",
      "userId": "04135816780688131851"
     },
     "user_tz": -330
    },
    "id": "n4bp6qTiL2nq",
    "outputId": "a6a8ff39-4ce9-4cdd-dcd9-efe8aaf4075f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5434198379516602\n"
     ]
    }
   ],
   "source": [
    "cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
    "\n",
    "print(f'{cosine_scores[0][0].item()}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMqa5ZM83Q6aGxzGbgwKJUM",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
